{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Uz6_xYjrkwuY"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Improved Balanced Random Forest (iBRF) with controllable SMOTE/RUS split.\n",
        "Per-tree pipeline: NC -> partial SMOTE (balance_split) -> RUS\n",
        "Author: Asif Newaz\n",
        "\"\"\"\n",
        "\n",
        "import numbers\n",
        "from warnings import warn\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "from numpy import float32 as DTYPE\n",
        "from numpy import float64 as DOUBLE\n",
        "from scipy.sparse import issparse\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from sklearn.base import clone\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble._base import _set_random_states\n",
        "from sklearn.ensemble._forest import _get_n_samples_bootstrap\n",
        "from sklearn.ensemble._forest import _parallel_build_trees\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.utils import check_array\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.utils.validation import _check_sample_weight\n",
        "from sklearn.utils.validation import validate_data  # sklearn >=1.6 public API\n",
        "\n",
        "from imblearn.base import BaseSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler, NeighbourhoodCleaningRule\n",
        "from imblearn.utils import check_target_type\n",
        "from sklearn.utils import check_X_y\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def _class_counts(y):\n",
        "    \"\"\"Return dict {label: count} preserving label dtype.\"\"\"\n",
        "    uniq, cnt = np.unique(y, return_counts=True)\n",
        "    return {label: int(c) for label, c in zip(uniq, cnt)}\n",
        "\n",
        "def _majority_label(counts):\n",
        "    \"\"\"Return the label with the maximum count (ties resolved by first occurrence).\"\"\"\n",
        "    return max(counts.items(), key=lambda kv: kv[1])[0]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Composite sampler with tracking and balance split\n",
        "# -----------------------------\n",
        "class NCRUSSMOTE(BaseSampler):\n",
        "    \"\"\"\n",
        "    Composite sampler: NC -> partial SMOTE -> RUS\n",
        "\n",
        "    - After NC, let n_M be the size of the (single) majority class and n_c the size\n",
        "      of any non-majority class c. We set SMOTE targets as:\n",
        "\n",
        "          n_c_target = n_c + balance_split * (n_M - n_c)\n",
        "\n",
        "      rounded to int, for each non-majority class. Majority class is untouched by SMOTE.\n",
        "\n",
        "    - Then RUS reduces ONLY the majority class down to:\n",
        "          max_c n_c_target\n",
        "      while leaving other classes unchanged.\n",
        "\n",
        "    This lets SMOTE handle only a fraction of the gap, with the remainder handled by RUS.\n",
        "    \"\"\"\n",
        "\n",
        "    _sampling_type = \"over-sampling\"\n",
        "    _parameter_constraints: dict = {}  # for sklearn >=1.5 compatibility\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        balance_split=0.65,           # fraction of gap filled by SMOTE (0..1)\n",
        "        random_state=None,\n",
        "        n_jobs=-1,\n",
        "        smote=None,\n",
        "        rus=None,\n",
        "        nc=None,\n",
        "        verbose=False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if not (0.0 <= balance_split <= 1.0):\n",
        "            raise ValueError(\"balance_split must be in [0, 1].\")\n",
        "        self.balance_split = float(balance_split)\n",
        "\n",
        "        self.random_state = random_state\n",
        "        self.n_jobs = n_jobs\n",
        "        self.smote = smote\n",
        "        self.rus = rus\n",
        "        self.nc = nc\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # per-instance stats (for this sampler / tree)\n",
        "        self.stats_ = {\"nc_removed\": 0, \"smote_generated\": 0, \"rus_removed\": 0}\n",
        "\n",
        "    def _validate_estimators(self):\n",
        "        # NC (supports n_jobs)\n",
        "        if self.nc is not None:\n",
        "            if not isinstance(self.nc, NeighbourhoodCleaningRule):\n",
        "                raise ValueError(f\"nc must be NeighbourhoodCleaningRule, got {type(self.nc)}.\")\n",
        "            self.nc_ = clone(self.nc)\n",
        "        else:\n",
        "            self.nc_ = NeighbourhoodCleaningRule(sampling_strategy=\"auto\", n_jobs=self.n_jobs)\n",
        "\n",
        "        # SMOTE (we'll pass a dict strategy per-batch; keep an instance prototype)\n",
        "        if self.smote is not None:\n",
        "            if not isinstance(self.smote, SMOTE):\n",
        "                raise ValueError(f\"smote must be SMOTE, got {type(self.smote)}.\")\n",
        "            self.smote_proto_ = clone(self.smote)\n",
        "        else:\n",
        "            # Use a prototype; we will clone with dict strategy at call time\n",
        "            try:\n",
        "                self.smote_proto_ = SMOTE(random_state=self.random_state, n_jobs=self.n_jobs)\n",
        "            except TypeError:\n",
        "                self.smote_proto_ = SMOTE(random_state=self.random_state)\n",
        "\n",
        "        # RUS (we will pass a dict strategy per-batch)\n",
        "        if self.rus is not None:\n",
        "            if not isinstance(self.rus, RandomUnderSampler):\n",
        "                raise ValueError(f\"rus must be RandomUnderSampler, got {type(self.rus)}.\")\n",
        "            self.rus_proto_ = clone(self.rus)\n",
        "        else:\n",
        "            self.rus_proto_ = RandomUnderSampler()\n",
        "\n",
        "    def _fit_resample(self, X, y):\n",
        "        self._validate_estimators()\n",
        "        y = check_target_type(y)\n",
        "        X, y = check_X_y(X, y, accept_sparse=[\"csr\", \"csc\"])\n",
        "\n",
        "        n0 = len(y)\n",
        "\n",
        "        # 1) Neighbourhood Cleaning\n",
        "        X_nc, y_nc = self.nc_.fit_resample(X, y)\n",
        "        n1 = len(y_nc)\n",
        "        self.stats_[\"nc_removed\"] = int(n0 - n1)\n",
        "\n",
        "        # 2) Partial SMOTE toward balance by balance_split\n",
        "        counts_nc = _class_counts(y_nc)\n",
        "        maj_label = _majority_label(counts_nc)\n",
        "        nM = counts_nc[maj_label]\n",
        "\n",
        "        # Build SMOTE target dict for NON-majority classes only\n",
        "        smote_target = {}\n",
        "        smote_generated_total = 0\n",
        "        for lbl, ncnt in counts_nc.items():\n",
        "            if lbl == maj_label:\n",
        "                continue\n",
        "            target = int(round(ncnt + self.balance_split * (nM - ncnt)))\n",
        "            # ensure monotonic (at least current size)\n",
        "            target = max(target, ncnt)\n",
        "            smote_target[lbl] = target\n",
        "            smote_generated_total += (target - ncnt)\n",
        "\n",
        "        if smote_target:\n",
        "            # Clone with per-batch dict strategy\n",
        "            try:\n",
        "                smote_ = clone(self.smote_proto_).set_params(sampling_strategy=smote_target)\n",
        "            except (TypeError, ValueError):\n",
        "                # older imblearn may require passing in ctor\n",
        "                try:\n",
        "                    smote_ = SMOTE(sampling_strategy=smote_target, random_state=self.random_state)\n",
        "                except TypeError:\n",
        "                    smote_ = SMOTE(sampling_strategy=smote_target, random_state=self.random_state)\n",
        "            X_sm, y_sm = smote_.fit_resample(X_nc, y_nc)\n",
        "        else:\n",
        "            # Degenerate (everything already equal)\n",
        "            X_sm, y_sm = X_nc, y_nc\n",
        "\n",
        "        n2 = len(y_sm)\n",
        "        # cross-check/record generated count (prefer computed target sum for stability)\n",
        "        self.stats_[\"smote_generated\"] = int(smote_generated_total)\n",
        "\n",
        "        # 3) RUS to finish the balance: reduce majority to max(minority targets)\n",
        "        counts_sm = _class_counts(y_sm)\n",
        "        maj_label_after = _majority_label(counts_sm)  # usually same, but recompute\n",
        "        max_minority_after = max([cnt for lbl, cnt in counts_sm.items() if lbl != maj_label_after], default=counts_sm[maj_label_after])\n",
        "\n",
        "        # Build RUS dict: keep all non-majority at their current counts; reduce majority to max minority\n",
        "        rus_target = {}\n",
        "        for lbl, cnt in counts_sm.items():\n",
        "            if lbl == maj_label_after:\n",
        "                rus_target[lbl] = int(max_minority_after)\n",
        "            else:\n",
        "                rus_target[lbl] = int(cnt)\n",
        "\n",
        "        rus_ = clone(self.rus_proto_).set_params(sampling_strategy=rus_target)\n",
        "        X_res, y_res = rus_.fit_resample(X_sm, y_sm)\n",
        "        n3 = len(y_res)\n",
        "\n",
        "        # Only majority can be reduced; compute removed count as delta in majority\n",
        "        rus_removed = max(0, counts_sm[maj_label_after] - rus_target[maj_label_after])\n",
        "        self.stats_[\"rus_removed\"] = int(rus_removed)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f\"[NCRUSSMOTE] NC removed: {self.stats_['nc_removed']}, \"\n",
        "                f\"SMOTE generated: {self.stats_['smote_generated']}, \"\n",
        "                f\"RUS removed: {self.stats_['rus_removed']}\"\n",
        "            )\n",
        "\n",
        "        return X_res, y_res\n",
        "\n",
        "    def get_sampling_report(self):\n",
        "        \"\"\"Return a dict with counts of removed/generated samples for this sampler instance.\"\"\"\n",
        "        return dict(self.stats_)\n",
        "\n",
        "\n",
        "# ---------------------------------------\n",
        "# Internal helper for parallel tree build\n",
        "# ---------------------------------------\n",
        "def _local_parallel_build_trees(\n",
        "    sampler,\n",
        "    tree,\n",
        "    forest,\n",
        "    X,\n",
        "    y,\n",
        "    sample_weight,\n",
        "    tree_idx,\n",
        "    n_trees,\n",
        "    verbose=0,\n",
        "    class_weight=None,\n",
        "    n_samples_bootstrap=None,\n",
        "):\n",
        "    # Resample (NC -> partial SMOTE -> RUS) for this tree\n",
        "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
        "\n",
        "    # Ensure bootstrap doesn't exceed resampled size\n",
        "    if _get_n_samples_bootstrap is not None:\n",
        "        n_samples_bootstrap = min(n_samples_bootstrap, X_resampled.shape[0])\n",
        "\n",
        "    # Build tree on the resampled set\n",
        "    tree = _parallel_build_trees(\n",
        "        tree,\n",
        "        forest,\n",
        "        X_resampled,\n",
        "        y_resampled,\n",
        "        sample_weight=None,  # synthetic samples break direct mapping of original weights\n",
        "        tree_idx=tree_idx,\n",
        "        n_trees=n_trees,\n",
        "        verbose=verbose,\n",
        "        class_weight=class_weight,\n",
        "        n_samples_bootstrap=n_samples_bootstrap,\n",
        "    )\n",
        "    return sampler, tree\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# The iBRF classifier (drop-in estimator)\n",
        "# --------------------------------------\n",
        "class ImprovedBalancedRandomForestClassifier(RandomForestClassifier):\n",
        "    \"\"\"\n",
        "    iBRF: For each tree, apply NC -> partial SMOTE (balance_split) -> RUS on that tree's\n",
        "    effective training subset, then fit the tree. OOB is not supported.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_estimators=100,\n",
        "        *,\n",
        "        criterion=\"gini\",\n",
        "        max_depth=None,\n",
        "        min_samples_split=2,\n",
        "        min_samples_leaf=1,\n",
        "        min_weight_fraction_leaf=0.0,\n",
        "        max_features=\"sqrt\",  # 'auto' deprecated; 'sqrt' is the modern default for classification\n",
        "        max_leaf_nodes=None,\n",
        "        min_impurity_decrease=0.0,\n",
        "        bootstrap=True,\n",
        "        oob_score=False,  # forbidden (SMOTE makes OOB undefined)\n",
        "        balance_split=0.65,   # <-- key control knob (0..1)\n",
        "        n_jobs=None,\n",
        "        random_state=None,\n",
        "        verbose=0,\n",
        "        warm_start=False,\n",
        "        class_weight=None,\n",
        "        ccp_alpha=0.0,\n",
        "        max_samples=None,\n",
        "        smote=None,\n",
        "        rus=None,\n",
        "        nc=None,\n",
        "        sampler_verbose=False,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            criterion=criterion,\n",
        "            max_depth=max_depth,\n",
        "            n_estimators=n_estimators,\n",
        "            bootstrap=bootstrap,\n",
        "            oob_score=oob_score,\n",
        "            n_jobs=n_jobs,\n",
        "            random_state=random_state,\n",
        "            verbose=verbose,\n",
        "            warm_start=warm_start,\n",
        "            class_weight=class_weight,\n",
        "            min_samples_split=min_samples_split,\n",
        "            min_samples_leaf=min_samples_leaf,\n",
        "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
        "            max_features=max_features,\n",
        "            max_leaf_nodes=max_leaf_nodes,\n",
        "            min_impurity_decrease=min_impurity_decrease,\n",
        "            ccp_alpha=ccp_alpha,\n",
        "            max_samples=max_samples,\n",
        "        )\n",
        "        self.balance_split = float(balance_split)\n",
        "        self.random_state = random_state\n",
        "        self.n_jobs = n_jobs\n",
        "\n",
        "        self.smote = smote\n",
        "        self.rus = rus\n",
        "        self.nc = nc\n",
        "        self.sampler_verbose = sampler_verbose\n",
        "\n",
        "        # storage\n",
        "        self.base_sampler_ = None\n",
        "        self.samplers_ = []   # one per fitted tree\n",
        "\n",
        "    def _validate_estimator(self, default=DecisionTreeClassifier()):\n",
        "        \"\"\"Validate n_estimators and set base estimator + base sampler prototypes.\"\"\"\n",
        "        if not isinstance(self.n_estimators, (numbers.Integral, np.integer)):\n",
        "            raise ValueError(f\"n_estimators must be an integer, got {type(self.n_estimators)}.\")\n",
        "        if self.n_estimators <= 0:\n",
        "            raise ValueError(f\"n_estimators must be greater than zero, got {self.n_estimators}.\")\n",
        "\n",
        "        # Base tree\n",
        "        be = getattr(self, \"base_estimator\", None)\n",
        "        self.base_estimator_ = clone(be) if be is not None else clone(default)\n",
        "\n",
        "        # Prototype sampler used to clone per-tree samplers\n",
        "        self.base_sampler_ = NCRUSSMOTE(\n",
        "            balance_split=self.balance_split,\n",
        "            random_state=self.random_state,\n",
        "            n_jobs=self.n_jobs,\n",
        "            smote=self.smote,\n",
        "            rus=self.rus,\n",
        "            nc=self.nc,\n",
        "            verbose=self.sampler_verbose,\n",
        "        )\n",
        "\n",
        "    def _make_sampler_estimator(self, random_state=None):\n",
        "        \"\"\"Instantiate a fresh (estimator, sampler) pair for a tree.\"\"\"\n",
        "        estimator = clone(self.base_estimator_)\n",
        "        estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})\n",
        "\n",
        "        sampler = clone(self.base_sampler_)\n",
        "        if random_state is not None:\n",
        "            _set_random_states(estimator, random_state)\n",
        "            _set_random_states(sampler, random_state)\n",
        "        return estimator, sampler\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        \"\"\"Build the forest of trees with per-tree NC -> partial SMOTE -> RUS resampling.\"\"\"\n",
        "        if self.oob_score:\n",
        "            raise ValueError(\n",
        "                \"oob_score=True is not supported in iBRF because SMOTE introduces \"\n",
        "                \"synthetic samples that invalidate OOB accounting.\"\n",
        "            )\n",
        "\n",
        "        # sklearn >=1.6: use public validate_data; fall back if needed\n",
        "        try:\n",
        "            X, y = validate_data(self, X, y, multi_output=True, accept_sparse=\"csc\", dtype=DTYPE)\n",
        "        except TypeError:\n",
        "            X, y = self._validate_data(X, y, multi_output=True, accept_sparse=\"csc\", dtype=DTYPE)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = _check_sample_weight(sample_weight, X)\n",
        "        self._n_features = X.shape[1]\n",
        "\n",
        "        if issparse(X):\n",
        "            X.sort_indices()\n",
        "\n",
        "        y = np.atleast_1d(y)\n",
        "        if y.ndim == 2 and y.shape[1] == 1:\n",
        "            warn(\n",
        "                \"A column-vector y was passed when a 1d array was expected. \"\n",
        "                \"Please reshape y to (n_samples,), e.g., using ravel().\",\n",
        "                DataConversionWarning,\n",
        "                stacklevel=2,\n",
        "            )\n",
        "        if y.ndim == 1:\n",
        "            y = np.reshape(y, (-1, 1))\n",
        "\n",
        "        self.n_outputs_ = y.shape[1]\n",
        "        y_encoded, expanded_class_weight = self._validate_y_class_weight(y)\n",
        "\n",
        "        if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n",
        "            y_encoded = np.ascontiguousarray(y_encoded, dtype=DOUBLE)\n",
        "\n",
        "        if expanded_class_weight is not None:\n",
        "            if sample_weight is not None:\n",
        "                sample_weight = sample_weight * expanded_class_weight\n",
        "            else:\n",
        "                sample_weight = expanded_class_weight\n",
        "\n",
        "        # Bootstrap sample size per tree\n",
        "        n_samples_bootstrap = _get_n_samples_bootstrap(\n",
        "            n_samples=X.shape[0], max_samples=self.max_samples\n",
        "        )\n",
        "\n",
        "        self._validate_estimator()\n",
        "        rng = check_random_state(self.random_state)\n",
        "\n",
        "        # Reset containers (unless warm_start and adding more trees)\n",
        "        if not self.warm_start or not hasattr(self, \"estimators_\"):\n",
        "            self.estimators_ = []\n",
        "            self.samplers_ = []\n",
        "\n",
        "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
        "        if n_more_estimators < 0:\n",
        "            raise ValueError(\n",
        "                f\"n_estimators={self.n_estimators} must be >= \"\n",
        "                f\"len(estimators_)={len(self.estimators_)} when warm_start=True\"\n",
        "            )\n",
        "        elif n_more_estimators == 0:\n",
        "            warn(\"Warm-start called without increasing n_estimators; no new trees fitted.\")\n",
        "            return self\n",
        "\n",
        "        if self.warm_start and len(self.estimators_) > 0:\n",
        "            rng.randint(np.iinfo(np.int32).max, size=len(self.estimators_))\n",
        "\n",
        "        # Create (tree, sampler) pairs\n",
        "        trees = []\n",
        "        samplers = []\n",
        "        for _ in range(n_more_estimators):\n",
        "            t, s = self._make_sampler_estimator(random_state=rng)\n",
        "            trees.append(t)\n",
        "            samplers.append(s)\n",
        "\n",
        "        # Fit trees in parallel\n",
        "        samplers_trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n",
        "            delayed(_local_parallel_build_trees)(\n",
        "                s, t, self, X, y_encoded, sample_weight,\n",
        "                i, len(trees), verbose=self.verbose,\n",
        "                class_weight=self.class_weight,\n",
        "                n_samples_bootstrap=n_samples_bootstrap,\n",
        "            )\n",
        "            for i, (s, t) in enumerate(zip(samplers, trees))\n",
        "        )\n",
        "        samplers, trees = zip(*samplers_trees)\n",
        "\n",
        "        # Collect fitted components\n",
        "        self.estimators_.extend(trees)\n",
        "        self.samplers_.extend(samplers)\n",
        "\n",
        "        # Decapsulate classes_ for single-output\n",
        "        if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n",
        "            self.n_classes_ = self.n_classes_[0]\n",
        "            self.classes_ = self.classes_[0]\n",
        "\n",
        "        return self\n",
        "\n",
        "    # Convenience: aggregate per-forest sampling stats\n",
        "    def get_forest_sampling_report(self, reduce=\"sum\"):\n",
        "        \"\"\"\n",
        "        Aggregate NC/SMOTE/RUS counts across all fitted trees.\n",
        "        reduce='sum' | 'mean'\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"samplers_\") or len(self.samplers_) == 0:\n",
        "            return {\"nc_removed\": 0, \"smote_generated\": 0, \"rus_removed\": 0}\n",
        "\n",
        "        stats = np.array(\n",
        "            [\n",
        "                [\n",
        "                    s.stats_.get(\"nc_removed\", 0),\n",
        "                    s.stats_.get(\"smote_generated\", 0),\n",
        "                    s.stats_.get(\"rus_removed\", 0),\n",
        "                ]\n",
        "                for s in self.samplers_\n",
        "            ],\n",
        "            dtype=float,\n",
        "        )\n",
        "        if reduce == \"mean\":\n",
        "            agg = stats.mean(axis=0)\n",
        "        else:\n",
        "            agg = stats.sum(axis=0)\n",
        "\n",
        "        return {\n",
        "            \"nc_removed\": float(agg[0]),\n",
        "            \"smote_generated\": float(agg[1]),\n",
        "            \"rus_removed\": float(agg[2]),\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def n_features_(self):\n",
        "        \"\"\"Number of features when fitting the estimator (back-compat shim).\"\"\"\n",
        "        return getattr(self, \"n_features_in_\", getattr(self, \"_n_features\", None))\n",
        "\n",
        "    def _more_tags(self):\n",
        "        return {\"multioutput\": False, \"multilabel\": False}\n",
        "\n",
        "\n",
        "# Alias\n",
        "iBRF = ImprovedBalancedRandomForestClassifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing on sample dataset"
      ],
      "metadata": {
        "id": "O7n93LXFmnyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- deps\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "\n",
        "# import your iBRF class (from the file you created earlier)\n",
        "#from ibrf import iBRF  # iBRF is an alias to ImprovedBalancedRandomForestClassifier\n",
        "\n",
        "# --- data\n",
        "X, y = make_classification(\n",
        "    n_samples=6000,\n",
        "    n_features=20,\n",
        "    n_informative=6,\n",
        "    n_redundant=2,\n",
        "    n_clusters_per_class=2,\n",
        "    weights=[0.92, 0.08],  # imbalanced\n",
        "    flip_y=0.01,\n",
        "    random_state=42,\n",
        ")\n",
        "Xtr, Xte, ytr, yte = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "def eval_scores(name, clf, Xtr, ytr, Xte, yte):\n",
        "    clf.fit(Xtr, ytr)\n",
        "    yhat = clf.predict(Xte)\n",
        "\n",
        "    auc = roc_auc_score(yte, yhat)\n",
        "\n",
        "    mcc = matthews_corrcoef(yte, yhat)\n",
        "    gmean = geometric_mean_score(yte, yhat, average=\"binary\")\n",
        "\n",
        "    print(f\"{name:>6} | MCC: {mcc:0.4f} | G-mean: {gmean:0.4f} | ROC AUC: {auc:0.4f}\")\n",
        "    return {\"mcc\": mcc, \"gmean\": gmean, \"auc\": auc}\n",
        "\n",
        "# --- 1) BRF baseline\n",
        "brf = BalancedRandomForestClassifier(\n",
        "    n_estimators=200, random_state=42, n_jobs=-1\n",
        ")\n",
        "scores_brf = eval_scores(\"BRF\", brf, Xtr, ytr, Xte, yte)\n",
        "\n",
        "# --- 2) Proposed iBRF\n",
        "# balance_split=0.65 → ~65% of the gap filled by SMOTE, rest by RUS\n",
        "ibrf = iBRF(\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    oob_score=False,\n",
        "    balance_split=0.65,\n",
        "    sampler_verbose=False,\n",
        ")\n",
        "scores_ibrf = eval_scores(\"iBRF\", ibrf, Xtr, ytr, Xte, yte)\n",
        "\n",
        "# Optional: see how much data each stage changed (sum across forest)\n",
        "print(\"iBRF forest sampling (sum):\", ibrf.get_forest_sampling_report(\"sum\"))\n",
        "print(\"iBRF forest sampling (mean per tree):\", ibrf.get_forest_sampling_report(\"mean\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVszbKQkmhCz",
        "outputId": "fb4a7a07-3adf-430a-deb9-ff2a178ece25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   BRF | MCC: 0.6528 | G-mean: 0.8678 | ROC AUC: 0.8714\n",
            "  iBRF | MCC: 0.7527 | G-mean: 0.8729 | ROC AUC: 0.8787\n",
            "iBRF forest sampling (sum): {'nc_removed': 55600.0, 'smote_generated': 419400.0, 'rus_removed': 225800.0}\n",
            "iBRF forest sampling (mean per tree): {'nc_removed': 278.0, 'smote_generated': 2097.0, 'rus_removed': 1129.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning for balance split ratio"
      ],
      "metadata": {
        "id": "A_mED1TxnSZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grid_search_balance_split_mcc.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_validate\n",
        "from sklearn.metrics import (\n",
        "    matthews_corrcoef,\n",
        "    roc_auc_score,\n",
        "    recall_score,\n",
        "    confusion_matrix,\n",
        "    make_scorer,\n",
        ")\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "#from ibrf import iBRF  # your ImprovedBalancedRandomForestClassifier alias\n",
        "\n",
        "# ---------------- Config ----------------\n",
        "#CSV_PATH = \"yeast4.csv\"   # last column is y (binary)\n",
        "N_SPLITS = 5\n",
        "RANDOM_STATE = 42\n",
        "N_ESTIMATORS = 100\n",
        "N_JOBS = -1\n",
        "BALANCE_SPLIT_GRID = [0.60, 0.70, 0.80]  # tune range\n",
        "REFIT_METRIC = \"MCC\"  # use MCC as optimization target\n",
        "# ----------------------------------------\n",
        "\n",
        "# --- Load dataset ---\n",
        "X, y = make_classification(\n",
        "        n_samples=6000,\n",
        "        n_features=20,\n",
        "        n_informative=6,\n",
        "        n_redundant=2,\n",
        "        n_clusters_per_class=2,\n",
        "        weights=[0.92, 0.08],\n",
        "        flip_y=0.01,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "# --- Custom scorers ---\n",
        "def specificity(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "scorers = {\n",
        "    \"MCC\": make_scorer(matthews_corrcoef),\n",
        "    \"Gmean\": make_scorer(geometric_mean_score, average=\"binary\"),\n",
        "    \"AUC\": make_scorer(roc_auc_score),\n",
        "    \"Sensitivity\": make_scorer(recall_score),\n",
        "    \"Specificity\": make_scorer(specificity),\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# --- iBRF: grid search on balance_split ---\n",
        "ibrf_base = iBRF(\n",
        "    n_estimators=N_ESTIMATORS,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=N_JOBS,\n",
        "    oob_score=False,\n",
        "    sampler_verbose=False,\n",
        ")\n",
        "\n",
        "param_grid = {\"balance_split\": BALANCE_SPLIT_GRID}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=ibrf_base,\n",
        "    param_grid=param_grid,\n",
        "    scoring=scorers,\n",
        "    refit=REFIT_METRIC,  # MCC as best metric\n",
        "    cv=cv,\n",
        "    n_jobs=N_JOBS,\n",
        "    return_train_score=False,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "print(\"Running GridSearchCV for iBRF (balance_split tuning, refit=MCC)...\")\n",
        "grid.fit(X, y)\n",
        "\n",
        "# --- Summarize results ---\n",
        "cvres = pd.DataFrame(grid.cv_results_)\n",
        "cols = [\"param_balance_split\"] + [\n",
        "    c for c in cvres.columns if c.startswith(\"mean_test_\") or c.startswith(\"std_test_\")\n",
        "]\n",
        "cv_table = cvres[cols].sort_values(by=\"mean_test_MCC\", ascending=False)\n",
        "\n",
        "print(\"\\n===== iBRF balance_split grid results (sorted by mean MCC) =====\")\n",
        "print(cv_table.to_string(index=False, float_format=lambda x: f\"{x:.4f}\"))\n",
        "\n",
        "best_split = grid.best_params_[\"balance_split\"]\n",
        "print(f\"\\nBest balance_split by MCC: {best_split:.2f}\")\n",
        "print(\"Best iBRF CV scores:\")\n",
        "for metric in scorers.keys():\n",
        "    mean_val = cvres.loc[grid.best_index_, f\"mean_test_{metric}\"]\n",
        "    std_val = cvres.loc[grid.best_index_, f\"std_test_{metric}\"]\n",
        "    print(f\"  {metric:>11}: {mean_val:.4f} ± {std_val:.4f}\")\n",
        "\n",
        "# --- Compare against BRF baseline ---\n",
        "print(\"\\nEvaluating BRF baseline with same CV and metrics...\")\n",
        "brf = BalancedRandomForestClassifier(\n",
        "    n_estimators=N_ESTIMATORS, random_state=RANDOM_STATE, n_jobs=N_JOBS\n",
        ")\n",
        "brf_results = cross_validate(brf, X, y, cv=cv, scoring=scorers, n_jobs=N_JOBS)\n",
        "\n",
        "def summarize(cv_results, name):\n",
        "    print(f\"\\n===== {name} 5-Fold Results =====\")\n",
        "    for key, values in cv_results.items():\n",
        "        if not key.startswith(\"test_\"):\n",
        "            continue\n",
        "        metric = key.replace(\"test_\", \"\")\n",
        "        scores = np.array(values, dtype=float)\n",
        "        print(f\"{metric:>12}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
        "\n",
        "summarize(brf_results, \"Balanced RF (BRF)\")\n",
        "\n",
        "print(f\"\\n✅ Final selected balance_split (MCC-best): {best_split:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqKMyAx4mzjo",
        "outputId": "662ce089-b895-4df5-83b9-0a27ed64ca76"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running GridSearchCV for iBRF (balance_split tuning, refit=MCC)...\n",
            "\n",
            "===== iBRF balance_split grid results (sorted by mean MCC) =====\n",
            " param_balance_split  mean_test_MCC  std_test_MCC  mean_test_Gmean  std_test_Gmean  mean_test_AUC  std_test_AUC  mean_test_Sensitivity  std_test_Sensitivity  mean_test_Specificity  std_test_Specificity\n",
            "              0.6000         0.7259        0.0485           0.8720          0.0214         0.8773        0.0197                 0.7826                0.0342                 0.9720                0.0065\n",
            "              0.8000         0.7202        0.0481           0.8583          0.0263         0.8656        0.0233                 0.7564                0.0437                 0.9747                0.0062\n",
            "              0.7000         0.7199        0.0421           0.8634          0.0204         0.8698        0.0184                 0.7665                0.0336                 0.9731                0.0063\n",
            "\n",
            "Best balance_split by MCC: 0.60\n",
            "Best iBRF CV scores:\n",
            "          MCC: 0.7259 ± 0.0485\n",
            "        Gmean: 0.8720 ± 0.0214\n",
            "          AUC: 0.8773 ± 0.0197\n",
            "  Sensitivity: 0.7826 ± 0.0342\n",
            "  Specificity: 0.9720 ± 0.0065\n",
            "\n",
            "Evaluating BRF baseline with same CV and metrics...\n",
            "\n",
            "===== Balanced RF (BRF) 5-Fold Results =====\n",
            "         MCC: 0.6602 ± 0.0564\n",
            "       Gmean: 0.8874 ± 0.0233\n",
            "         AUC: 0.8892 ± 0.0224\n",
            " Sensitivity: 0.8349 ± 0.0353\n",
            " Specificity: 0.9435 ± 0.0125\n",
            "\n",
            "✅ Final selected balance_split (MCC-best): 0.60\n"
          ]
        }
      ]
    }
  ]
}