{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ba10bd0c",
   "metadata": {},
   "source": [
    "Source Code for the paper titled - \"iBRF: Improved Balanced Random Forest Classifier\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "54174079",
   "metadata": {},
   "source": [
    "The code for the iBRF classifier is written using Sklearn and imblearn libraries (https://imbalanced-learn.org/stable/)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b50ffa6e",
   "metadata": {},
   "source": [
    "The parameters of the original libraries (SMOTE, RUS, NC, BRF, RF) are available in the implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ca76c",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38086bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "from warnings import warn\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import float32 as DTYPE\n",
    "from numpy import float64 as DOUBLE\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble._base import _set_random_states\n",
    "from sklearn.ensemble._forest import _get_n_samples_bootstrap\n",
    "from sklearn.ensemble._forest import _parallel_build_trees\n",
    "from sklearn.ensemble._forest import _generate_unsampled_indices\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import _safe_indexing\n",
    "from sklearn.utils.fixes import _joblib_parallel_args\n",
    "from sklearn.utils.validation import _check_sample_weight\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling.base import BaseUnderSampler\n",
    "from imblearn.over_sampling.base import BaseOverSampler\n",
    "from imblearn.base import BaseSampler\n",
    "\n",
    "from imblearn.utils import Substitution\n",
    "from imblearn.utils._docstring import _n_jobs_docstring\n",
    "from imblearn.utils._docstring import _random_state_docstring\n",
    "from imblearn.utils._validation import check_sampling_strategy\n",
    "from imblearn.utils._validation import _deprecate_positional_args\n",
    "\n",
    "#from smote_nc import SMOTENC\n",
    "#from smote_rus import SMOTERUS\n",
    "\n",
    "MAX_INT = np.iinfo(np.int32).max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8aa655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Hybrid sampling using SMOTE, RUS and NC\"\"\"\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import check_X_y\n",
    "\n",
    "from imblearn.base import BaseSampler\n",
    "#from imblearn.base import BaseCleaningSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling.base import BaseOverSampler\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from imblearn.utils import check_target_type\n",
    "from imblearn.utils import Substitution\n",
    "from imblearn.utils._docstring import _n_jobs_docstring\n",
    "from imblearn.utils._docstring import _random_state_docstring\n",
    "from imblearn.utils._validation import _deprecate_positional_args\n",
    "\n",
    "\n",
    "@Substitution(\n",
    "    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,\n",
    "    n_jobs=_n_jobs_docstring,\n",
    "    random_state=_random_state_docstring,\n",
    ")\n",
    "class SMOTENCRUS(BaseSampler):\n",
    "    \n",
    "    \"\"\"Over-sampling using SMOTE, undersampling using RUS and cleaning using ENN.\n",
    "    Combine over- and under-sampling using SMOTE, RUS, and Edited Nearest Neighbours.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    {sampling_strategy}\n",
    "    {random_state}\n",
    "    \n",
    "    smote : sampler object, default=None\n",
    "        The :class:`~imblearn.over_sampling.SMOTE` object to use. If not given,\n",
    "        a :class:`~imblearn.over_sampling.SMOTE` object with default parameters\n",
    "        will be given.\n",
    "    Attributes\n",
    "    ----------\n",
    "    sampling_strategy_ : dict\n",
    "        Dictionary containing the information to sample the dataset. The keys\n",
    "        corresponds to the class labels from which to sample and the values\n",
    "        are the number of samples to sample.\n",
    "    smote_ : sampler object\n",
    "        The validated :class:`~imblearn.over_sampling.SMOTE` instance.\n",
    "    nc_ : sampler object\n",
    "        The validated :class:`~imblearn.under_sampling.NeighbourhoodCleaningRule`\n",
    "        instance.\n",
    "    rus_ : sampler object\n",
    "        The validated :class:`~imblearn.under_sampling.RandomUnderSampler`\n",
    "        instance.\n",
    "    n_features_in_ : int\n",
    "        Number of features in the input dataset.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    _sampling_type = \"over-sampling\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        sampling_strategy=\"auto\",\n",
    "        random_state=None,\n",
    "        smote=None,\n",
    "        rus= None,\n",
    "        nc= None,\n",
    "        n_jobs=-1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.random_state = random_state\n",
    "        self.smote = smote\n",
    "        self.rus= rus\n",
    "        self.nc= nc\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _validate_estimator(self):\n",
    "        \n",
    "        if self.smote is not None:\n",
    "            if isinstance(self.smote, SMOTE):\n",
    "                self.smote_ = clone(self.smote)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"smote needs to be a SMOTE object.\"\n",
    "                    f\"Got {type(self.smote)} instead.\"\n",
    "                )\n",
    "        # Otherwise create a default SMOTE\n",
    "        else:\n",
    "            self.smote_ = SMOTE(\n",
    "                sampling_strategy=self.sampling_strategy,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=self.n_jobs,\n",
    "            )\n",
    "\n",
    "            \n",
    "        if self.rus is not None:\n",
    "            if isinstance(self.rus, RandomUnderSampler):\n",
    "                self.rus_ = clone(self.rus)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"enn needs to be an RUS.\"\n",
    "                    f\" Got {type(self.rus)} instead.\"\n",
    "                )\n",
    "        # Otherwise create a default RUS\n",
    "        else:\n",
    "            self.rus_ = RandomUnderSampler(\n",
    "                sampling_strategy=\"auto\", n_jobs=self.n_jobs\n",
    "            )\n",
    "            \n",
    "            \n",
    "        if self.nc is not None:\n",
    "            if isinstance(self.nc, NeighbourhoodCleaningRule):\n",
    "                self.nc_ = clone(self.nc)\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"enn needs to be an NeighbourhoodCleaningRule.\"\n",
    "                    f\" Got {type(self.rus)} instead.\"\n",
    "                )\n",
    "        # Otherwise create a default NC\n",
    "        else:\n",
    "            self.nc_ = NeighbourhoodCleaningRule(\n",
    "                sampling_strategy=\"auto\", n_jobs=self.n_jobs\n",
    "            )\n",
    "        \n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        self._validate_estimator()\n",
    "        y = check_target_type(y)\n",
    "        X, y = check_X_y(X, y, accept_sparse=[\"csr\", \"csc\"])\n",
    "        self.sampling_strategy_ = self.sampling_strategy\n",
    "\n",
    "        X_res, y_res = self.nc_.fit_resample(X, y)\n",
    "        X_res, y_res = self.rus_.fit_resample(X_res, y_res)\n",
    "                                              \n",
    "        return self.smote_.fit_resample(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b030b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the forest\n",
    "\n",
    "def _local_parallel_build_trees(\n",
    "    sampler,\n",
    "    tree,\n",
    "    forest,\n",
    "    X,\n",
    "    y,\n",
    "    sample_weight,\n",
    "    tree_idx,\n",
    "    n_trees,\n",
    "    verbose=0,\n",
    "    class_weight=None,\n",
    "    n_samples_bootstrap=None,\n",
    "):\n",
    "    # resample before to fit the tree\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = _safe_indexing(sample_weight, sampler.sample_indices_)\n",
    "    if _get_n_samples_bootstrap is not None:\n",
    "        n_samples_bootstrap = min(n_samples_bootstrap, X_resampled.shape[0])\n",
    "    tree = _parallel_build_trees(\n",
    "        tree,\n",
    "        forest,\n",
    "        X_resampled,\n",
    "        y_resampled,\n",
    "        sample_weight,\n",
    "        tree_idx,\n",
    "        n_trees,\n",
    "        verbose=verbose,\n",
    "        class_weight=class_weight,\n",
    "        n_samples_bootstrap=n_samples_bootstrap,\n",
    "    )\n",
    "    return sampler, tree\n",
    "\n",
    "\n",
    "@Substitution(\n",
    "    sampling_strategy=BaseOverSampler._sampling_strategy_docstring,\n",
    "    n_jobs=_n_jobs_docstring,\n",
    "    random_state=_random_state_docstring,\n",
    ")\n",
    "class snrBalancedRandomForestClassifier(RandomForestClassifier):\n",
    "    \"\"\"An improved balanced random forest classifier.\n",
    "\n",
    "    A balanced random forest randomly under-samples each boostrap sample to\n",
    "    balance it. The iBRF approach uses hybrid sampling to balance it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators : int, default=100\n",
    "        The number of trees in the forest.\n",
    "\n",
    "    criterion : {{\"gini\", \"entropy\"}}, default=\"gini\"\n",
    "        The function to measure the quality of a split. Supported criteria are\n",
    "        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "        Note: this parameter is tree-specific.\n",
    "\n",
    "    max_depth : int, default=None\n",
    "        The maximum depth of the tree. If None, then nodes are expanded until\n",
    "        all leaves are pure or until all leaves contain less than\n",
    "        min_samples_split samples.\n",
    "\n",
    "    min_samples_split : int or float, default=2\n",
    "        The minimum number of samples required to split an internal node:\n",
    "\n",
    "        - If int, then consider `min_samples_split` as the minimum number.\n",
    "        - If float, then `min_samples_split` is a percentage and\n",
    "          `ceil(min_samples_split * n_samples)` are the minimum\n",
    "          number of samples for each split.\n",
    "\n",
    "    min_samples_leaf : int or float, default=1\n",
    "        The minimum number of samples required to be at a leaf node:\n",
    "\n",
    "        - If int, then consider ``min_samples_leaf`` as the minimum number.\n",
    "        - If float, then ``min_samples_leaf`` is a fraction and\n",
    "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
    "          number of samples for each node.\n",
    "\n",
    "    min_weight_fraction_leaf : float, default=0.0\n",
    "        The minimum weighted fraction of the sum total of weights (of all\n",
    "        the input samples) required to be at a leaf node. Samples have\n",
    "        equal weight when sample_weight is not provided.\n",
    "\n",
    "    max_features : {{\"auto\", \"sqrt\", \"log2\"}}, int, float, or None, \\\n",
    "            default=\"auto\"\n",
    "        The number of features to consider when looking for the best split:\n",
    "\n",
    "        - If int, then consider `max_features` features at each split.\n",
    "        - If float, then `max_features` is a percentage and\n",
    "          `int(max_features * n_features)` features are considered at each\n",
    "          split.\n",
    "        - If \"auto\", then `max_features=sqrt(n_features)`.\n",
    "        - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
    "        - If \"log2\", then `max_features=log2(n_features)`.\n",
    "        - If None, then `max_features=n_features`.\n",
    "\n",
    "        Note: the search for a split does not stop until at least one\n",
    "        valid partition of the node samples is found, even if it requires to\n",
    "        effectively inspect more than ``max_features`` features.\n",
    "\n",
    "    max_leaf_nodes : int, default=None\n",
    "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
    "        Best nodes are defined as relative reduction in impurity.\n",
    "        If None then unlimited number of leaf nodes.\n",
    "\n",
    "    min_impurity_decrease : float, default=0.0\n",
    "        A node will be split if this split induces a decrease of the impurity\n",
    "        greater than or equal to this value.\n",
    "        The weighted impurity decrease equation is the following::\n",
    "\n",
    "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
    "                                - N_t_L / N_t * left_impurity)\n",
    "\n",
    "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
    "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
    "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
    "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
    "        if ``sample_weight`` is passed.\n",
    "\n",
    "    bootstrap : bool, default=True\n",
    "        Whether bootstrap samples are used when building trees.\n",
    "\n",
    "    oob_score : bool, default=False\n",
    "        Whether to use out-of-bag samples to estimate\n",
    "        the generalization accuracy.\n",
    "\n",
    "    {sampling_strategy}\n",
    "\n",
    "    replacement : bool, default=False\n",
    "        Whether or not to sample randomly with replacement or not.\n",
    "\n",
    "    {n_jobs}\n",
    "\n",
    "    {random_state}\n",
    "\n",
    "    verbose : int, default=0\n",
    "        Controls the verbosity of the tree building process.\n",
    "\n",
    "    warm_start : bool, default=False\n",
    "        When set to ``True``, reuse the solution of the previous call to fit\n",
    "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
    "        new forest.\n",
    "\n",
    "    class_weight : dict, list of dicts, {{\"balanced\", \"balanced_subsample\"}}, \\\n",
    "            default=None\n",
    "        Weights associated with classes in the form dictionary with the key\n",
    "        being the class_label and the value the weight.\n",
    "        If not given, all classes are supposed to have weight one. For\n",
    "        multi-output problems, a list of dicts can be provided in the same\n",
    "        order as the columns of y.\n",
    "        Note that for multioutput (including multilabel) weights should be\n",
    "        defined for each class of every column in its own dict. For example,\n",
    "        for four-class multilabel classification weights should be\n",
    "        [{{0: 1, 1: 1}}, {{0: 1, 1: 5}}, {{0: 1, 1: 1}}, {{0: 1, 1: 1}}]\n",
    "        instead of [{{1:1}}, {{2:5}}, {{3:1}}, {{4:1}}].\n",
    "        The \"balanced\" mode uses the values of y to automatically adjust\n",
    "        weights inversely proportional to class frequencies in the input data\n",
    "        as ``n_samples / (n_classes * np.bincount(y))``\n",
    "        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
    "        weights are computed based on the bootstrap sample for every tree\n",
    "        grown.\n",
    "        For multi-output, the weights of each column of y will be multiplied.\n",
    "        Note that these weights will be multiplied with sample_weight (passed\n",
    "        through the fit method) if sample_weight is specified.\n",
    "\n",
    "    ccp_alpha : non-negative float, default=0.0\n",
    "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
    "        subtree with the largest cost complexity that is smaller than\n",
    "        ``ccp_alpha`` will be chosen. By default, no pruning is performed.\n",
    "\n",
    "        .. versionadded:: 0.6\n",
    "           Added in `scikit-learn` in 0.22\n",
    "\n",
    "    max_samples : int or float, default=None\n",
    "        If bootstrap is True, the number of samples to draw from X\n",
    "        to train each base estimator.\n",
    "            - If None (default), then draw `X.shape[0]` samples.\n",
    "            - If int, then draw `max_samples` samples.\n",
    "            - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
    "              `max_samples` should be in the interval `(0, 1)`.\n",
    "        Be aware that the final number samples used will be the minimum between\n",
    "        the number of samples given in `max_samples` and the number of samples\n",
    "        obtained after resampling.\n",
    "\n",
    "        .. versionadded:: 0.6\n",
    "           Added in `scikit-learn` in 0.22\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    base_estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier` instance\n",
    "        The child estimator template used to create the collection of fitted\n",
    "        sub-estimators.\n",
    "\n",
    "    estimators_ : list of :class:`~sklearn.tree.DecisionTreeClassifier`\n",
    "        The collection of fitted sub-estimators.\n",
    "\n",
    "    base_sampler_ : :class:`~imblearn.under_sampling.RandomUnderSampler`\n",
    "        The base sampler used to construct the subsequent list of samplers.\n",
    "\n",
    "    samplers_ : list of :class:`~imblearn.under_sampling.RandomUnderSampler`\n",
    "        The collection of fitted samplers.\n",
    "\n",
    "    pipelines_ : list of Pipeline.\n",
    "        The collection of fitted pipelines (samplers + trees).\n",
    "\n",
    "    classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
    "        The classes labels (single output problem), or a list of arrays of\n",
    "        class labels (multi-output problem).\n",
    "\n",
    "    n_classes_ : int or list\n",
    "        The number of classes (single output problem), or a list containing the\n",
    "        number of classes for each output (multi-output problem).\n",
    "\n",
    "    n_features_ : int\n",
    "        The number of features when ``fit`` is performed.\n",
    "\n",
    "        .. deprecated:: 1.0\n",
    "           `n_features_` is deprecated in `scikit-learn` 1.0 and will be removed\n",
    "           in version 1.2. Depending of the version of `scikit-learn` installed,\n",
    "           you will get be warned or not.\n",
    "\n",
    "    n_features_in_ : int\n",
    "        Number of features in the input dataset.\n",
    "\n",
    "        .. versionadded:: 0.9\n",
    "\n",
    "    feature_names_in_ : ndarray of shape (n_features_in_,)\n",
    "        Names of features seen during `fit`. Defined only when `X` has feature\n",
    "        names that are all strings.\n",
    "\n",
    "        .. versionadded:: 0.9\n",
    "\n",
    "    n_outputs_ : int\n",
    "        The number of outputs when ``fit`` is performed.\n",
    "\n",
    "    feature_importances_ : ndarray of shape (n_features,)\n",
    "        The feature importances (the higher, the more important the feature).\n",
    "\n",
    "    oob_score_ : float\n",
    "        Score of the training dataset obtained using an out-of-bag estimate.\n",
    "\n",
    "    oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
    "        Decision function computed with out-of-bag estimate on the training\n",
    "        set. If n_estimators is small it might be possible that a data point\n",
    "        was never left out during the bootstrap. In this case,\n",
    "        `oob_decision_function_` might contain NaN.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Chen, Chao, Andy Liaw, and Leo Breiman. \"Using random forest to\n",
    "       learn imbalanced data.\" University of California, Berkeley 110 (2004):\n",
    "       1-12.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        *,\n",
    "        criterion=\"gini\",\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        min_weight_fraction_leaf=0.0,\n",
    "        max_features=\"auto\",\n",
    "        max_leaf_nodes=None,\n",
    "        min_impurity_decrease=0.0,\n",
    "        bootstrap=True,\n",
    "        oob_score=False,\n",
    "        sampling_strategy=\"auto\",\n",
    "        replacement=False,\n",
    "        n_jobs=None,\n",
    "        random_state=None,\n",
    "        verbose=0,\n",
    "        warm_start=False,\n",
    "        class_weight=None,\n",
    "        ccp_alpha=0.0,\n",
    "        max_samples=None,\n",
    "        k_neighbors= 5,\n",
    "        smote= None,\n",
    "        rus= None,\n",
    "        nc= None\n",
    "        \n",
    "    ):\n",
    "        super().__init__(\n",
    "            criterion=criterion,\n",
    "            max_depth=max_depth,\n",
    "            n_estimators=n_estimators,\n",
    "            bootstrap=bootstrap,\n",
    "            oob_score=oob_score,\n",
    "            n_jobs=n_jobs,\n",
    "            random_state=random_state,\n",
    "            verbose=verbose,\n",
    "            warm_start=warm_start,\n",
    "            class_weight=class_weight,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_features=max_features,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            ccp_alpha=ccp_alpha,\n",
    "            max_samples=max_samples,\n",
    "        )\n",
    "\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.random_state = random_state\n",
    "        self.k_neighbors=k_neighbors\n",
    "        self.n_jobs=n_jobs\n",
    "        self.replacement= replacement\n",
    "        self.smote= smote\n",
    "        self.rus= rus\n",
    "        self.nc= nc\n",
    "\n",
    "    def _validate_estimator(self, default=DecisionTreeClassifier()):\n",
    "        \"\"\"Check the estimator and the n_estimator attribute, set the\n",
    "        `base_estimator_` attribute.\"\"\"\n",
    "        if not isinstance(self.n_estimators, (numbers.Integral, np.integer)):\n",
    "            raise ValueError(\n",
    "                f\"n_estimators must be an integer, \" f\"got {type(self.n_estimators)}.\"\n",
    "            )\n",
    "\n",
    "        if self.n_estimators <= 0:\n",
    "            raise ValueError(\n",
    "                f\"n_estimators must be greater than zero, \" f\"got {self.n_estimators}.\"\n",
    "            )\n",
    "\n",
    "        if self.base_estimator is not None:\n",
    "            self.base_estimator_ = clone(self.base_estimator)\n",
    "        else:\n",
    "            self.base_estimator_ = clone(default)\n",
    "\n",
    "        self.base_sampler_ = SMOTENCRUS(\n",
    "            sampling_strategy= self._sampling_strategy,\n",
    "            random_state= self.random_state,\n",
    "            smote= self.smote,\n",
    "            rus= self.rus,\n",
    "            nc= self.nc,\n",
    "            n_jobs= self.n_jobs\n",
    "        )\n",
    "\n",
    "    def _make_sampler_estimator(self, random_state=None):\n",
    "        \"\"\"Make and configure a copy of the `base_estimator_` attribute.\n",
    "        Warning: This method should be used to properly instantiate new\n",
    "        sub-estimators.\n",
    "        \"\"\"\n",
    "        estimator = clone(self.base_estimator_)\n",
    "        estimator.set_params(**{p: getattr(self, p) for p in self.estimator_params})\n",
    "        sampler = clone(self.base_sampler_)\n",
    "\n",
    "        if random_state is not None:\n",
    "            _set_random_states(estimator, random_state)\n",
    "            _set_random_states(sampler, random_state)\n",
    "\n",
    "        return estimator, sampler\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"Build a forest of trees from the training set (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
    "            The training input samples. Internally, its dtype will be converted\n",
    "            to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
    "            converted into a sparse ``csc_matrix``.\n",
    "\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            The target values (class labels in classification, real numbers in\n",
    "            regression).\n",
    "\n",
    "        sample_weight : array-like of shape (n_samples,)\n",
    "            Sample weights. If None, then samples are equally weighted. Splits\n",
    "            that would create child nodes with net zero or negative weight are\n",
    "            ignored while searching for a split in each node. In the case of\n",
    "            classification, splits are also ignored if they would result in any\n",
    "            single class carrying a negative weight in either child node.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            The fitted instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate or convert input data\n",
    "        if issparse(y):\n",
    "            raise ValueError(\"sparse multilabel-indicator for y is not supported.\")\n",
    "        X, y = self._validate_data(\n",
    "            X, y, multi_output=True, accept_sparse=\"csc\", dtype=DTYPE\n",
    "        )\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = _check_sample_weight(sample_weight, X)\n",
    "        self._n_features = X.shape[1]\n",
    "\n",
    "        if issparse(X):\n",
    "            # Pre-sort indices to avoid that each individual tree of the\n",
    "            # ensemble sorts the indices.\n",
    "            X.sort_indices()\n",
    "\n",
    "        y = np.atleast_1d(y)\n",
    "        if y.ndim == 2 and y.shape[1] == 1:\n",
    "            warn(\n",
    "                \"A column-vector y was passed when a 1d array was\"\n",
    "                \" expected. Please change the shape of y to \"\n",
    "                \"(n_samples,), for example using ravel().\",\n",
    "                DataConversionWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "\n",
    "        if y.ndim == 1:\n",
    "            # reshape is necessary to preserve the data contiguity against vs\n",
    "            # [:, np.newaxis] that does not.\n",
    "            y = np.reshape(y, (-1, 1))\n",
    "\n",
    "        self.n_outputs_ = y.shape[1]\n",
    "\n",
    "        y_encoded, expanded_class_weight = self._validate_y_class_weight(y)\n",
    "\n",
    "        if getattr(y, \"dtype\", None) != DOUBLE or not y.flags.contiguous:\n",
    "            y_encoded = np.ascontiguousarray(y_encoded, dtype=DOUBLE)\n",
    "\n",
    "        if isinstance(self.sampling_strategy, dict):\n",
    "            self._sampling_strategy = {\n",
    "                np.where(self.classes_[0] == key)[0][0]: value\n",
    "                for key, value in check_sampling_strategy(\n",
    "                    self.sampling_strategy,\n",
    "                    y,\n",
    "                    \"under-sampling\",\n",
    "                ).items()\n",
    "            }\n",
    "        else:\n",
    "            self._sampling_strategy = self.sampling_strategy\n",
    "\n",
    "        if expanded_class_weight is not None:\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = sample_weight * expanded_class_weight\n",
    "            else:\n",
    "                sample_weight = expanded_class_weight\n",
    "\n",
    "        # Get bootstrap sample size\n",
    "        n_samples_bootstrap = _get_n_samples_bootstrap(\n",
    "            n_samples=X.shape[0], max_samples=self.max_samples\n",
    "        )\n",
    "\n",
    "        # Check parameters\n",
    "        self._validate_estimator()\n",
    "\n",
    "        if not self.bootstrap and self.oob_score:\n",
    "            raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
    "\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        if not self.warm_start or not hasattr(self, \"estimators_\"):\n",
    "            # Free allocated memory, if any\n",
    "            self.estimators_ = []\n",
    "            self.samplers_ = []\n",
    "            self.pipelines_ = []\n",
    "\n",
    "        n_more_estimators = self.n_estimators - len(self.estimators_)\n",
    "\n",
    "        if n_more_estimators < 0:\n",
    "            raise ValueError(\n",
    "                \"n_estimators=%d must be larger or equal to \"\n",
    "                \"len(estimators_)=%d when warm_start==True\"\n",
    "                % (self.n_estimators, len(self.estimators_))\n",
    "            )\n",
    "\n",
    "        elif n_more_estimators == 0:\n",
    "            warn(\n",
    "                \"Warm-start fitting without increasing n_estimators does not \"\n",
    "                \"fit new trees.\"\n",
    "            )\n",
    "        else:\n",
    "            if self.warm_start and len(self.estimators_) > 0:\n",
    "                # We draw from the random state to get the random state we\n",
    "                # would have got if we hadn't used a warm_start.\n",
    "                random_state.randint(MAX_INT, size=len(self.estimators_))\n",
    "\n",
    "            trees = []\n",
    "            samplers = []\n",
    "            for _ in range(n_more_estimators):\n",
    "                tree, sampler = self._make_sampler_estimator(random_state=random_state)\n",
    "                trees.append(tree)\n",
    "                samplers.append(sampler)\n",
    "\n",
    "            # Parallel loop: we prefer the threading backend as the Cython code\n",
    "            # for fitting the trees is internally releasing the Python GIL\n",
    "            # making threading more efficient than multiprocessing in\n",
    "            # that case. However, we respect any parallel_backend contexts set\n",
    "            # at a higher level, since correctness does not rely on using\n",
    "            # threads.\n",
    "            samplers_trees = Parallel(\n",
    "                n_jobs=self.n_jobs,\n",
    "                verbose=self.verbose,\n",
    "                **_joblib_parallel_args(prefer=\"threads\"),\n",
    "            )(\n",
    "                delayed(_local_parallel_build_trees)(\n",
    "                    s,\n",
    "                    t,\n",
    "                    self,\n",
    "                    X,\n",
    "                    y_encoded,\n",
    "                    sample_weight,\n",
    "                    i,\n",
    "                    len(trees),\n",
    "                    verbose=self.verbose,\n",
    "                    class_weight=self.class_weight,\n",
    "                    n_samples_bootstrap=n_samples_bootstrap,\n",
    "                )\n",
    "                for i, (s, t) in enumerate(zip(samplers, trees))\n",
    "            )\n",
    "            samplers, trees = zip(*samplers_trees)\n",
    "\n",
    "            # Collect newly grown trees\n",
    "            self.estimators_.extend(trees)\n",
    "            self.samplers_.extend(samplers)\n",
    "\n",
    "            # Create pipeline with the fitted samplers and trees\n",
    "            self.pipelines_.extend(\n",
    "                [\n",
    "                    make_pipeline(deepcopy(s), deepcopy(t))\n",
    "                    for s, t in zip(samplers, trees)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        if self.oob_score:\n",
    "            self._set_oob_score(X, y_encoded)\n",
    "\n",
    "        # Decapsulate classes_ attributes\n",
    "        if hasattr(self, \"classes_\") and self.n_outputs_ == 1:\n",
    "            self.n_classes_ = self.n_classes_[0]\n",
    "            self.classes_ = self.classes_[0]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _set_oob_score(self, X, y):\n",
    "        \"\"\"Compute out-of-bag score.\"\"\"\n",
    "        X = check_array(X, dtype=DTYPE, accept_sparse=\"csr\")\n",
    "\n",
    "        n_classes_ = self.n_classes_\n",
    "        n_samples = y.shape[0]\n",
    "\n",
    "        oob_decision_function = []\n",
    "        oob_score = 0.0\n",
    "        predictions = [\n",
    "            np.zeros((n_samples, n_classes_[k])) for k in range(self.n_outputs_)\n",
    "        ]\n",
    "\n",
    "        for sampler, estimator in zip(self.samplers_, self.estimators_):\n",
    "            X_resample = X[sampler.sample_indices_]\n",
    "            y_resample = y[sampler.sample_indices_]\n",
    "\n",
    "            n_sample_subset = y_resample.shape[0]\n",
    "            n_samples_bootstrap = _get_n_samples_bootstrap(\n",
    "                n_sample_subset, self.max_samples\n",
    "            )\n",
    "\n",
    "            unsampled_indices = _generate_unsampled_indices(\n",
    "                estimator.random_state, n_sample_subset, n_samples_bootstrap\n",
    "            )\n",
    "            p_estimator = estimator.predict_proba(\n",
    "                X_resample[unsampled_indices, :], check_input=False\n",
    "            )\n",
    "\n",
    "            if self.n_outputs_ == 1:\n",
    "                p_estimator = [p_estimator]\n",
    "\n",
    "            for k in range(self.n_outputs_):\n",
    "                indices = sampler.sample_indices_[unsampled_indices]\n",
    "                predictions[k][indices, :] += p_estimator[k]\n",
    "\n",
    "        for k in range(self.n_outputs_):\n",
    "            if (predictions[k].sum(axis=1) == 0).any():\n",
    "                warn(\n",
    "                    \"Some inputs do not have OOB scores. \"\n",
    "                    \"This probably means too few trees were used \"\n",
    "                    \"to compute any reliable oob estimates.\"\n",
    "                )\n",
    "\n",
    "            with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "                # with the resampling, we are likely to have rows not included\n",
    "                # for the OOB score leading to division by zero\n",
    "                decision = predictions[k] / predictions[k].sum(axis=1)[:, np.newaxis]\n",
    "            mask_scores = np.isnan(np.sum(decision, axis=1))\n",
    "            oob_decision_function.append(decision)\n",
    "            oob_score += np.mean(\n",
    "                y[~mask_scores, k] == np.argmax(predictions[k][~mask_scores], axis=1),\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "        if self.n_outputs_ == 1:\n",
    "            self.oob_decision_function_ = oob_decision_function[0]\n",
    "        else:\n",
    "            self.oob_decision_function_ = oob_decision_function\n",
    "\n",
    "        self.oob_score_ = oob_score / self.n_outputs_\n",
    "\n",
    "    @property\n",
    "    def n_features_(self):\n",
    "        \"\"\"Number of features when fitting the estimator.\"\"\"\n",
    "        return getattr(self.n_features_in_, \"n_features_\", self._n_features)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {\n",
    "            \"multioutput\": False,\n",
    "            \"multilabel\": False,\n",
    "        }"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49498663",
   "metadata": {},
   "source": [
    "implementation example\n",
    " _ _ _\n",
    " \n",
    "for i in [0.2, 0.3, 0.4]:\n",
    "    brf= snrBalancedRandomForestClassifier(n_jobs=-1, random_state=100, rus= RandomUnderSampler(sampling_strategy=i))\n",
    "    results = cross_validate(brf, x,y, cv=StratifiedKFold(5), scoring=scores)\n",
    "\n",
    "    print( 'sampling rate: {}'.format(i))\n",
    "    print(df.mean()*100)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10ced852",
   "metadata": {},
   "source": [
    "Examples\n",
    "    --------\n",
    "    >>> from collections import Counter\n",
    "    >>> from sklearn.datasets import make_classification\n",
    "    >>> from imblearn.combine import SMOTEENN \n",
    "    \n",
    "    >>> X, y = make_classification(n_classes=2, class_sep=2,\n",
    "    ... weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
    "    ... n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
    "    \n",
    "    >>> print('Original dataset shape %s' % Counter(y))\n",
    "    Original dataset shape Counter({{1: 900, 0: 100}})\n",
    "    \n",
    "    >>> sme = SMOTEENN(random_state=42)\n",
    "    >>> X_res, y_res = sme.fit_resample(X, y)\n",
    "    >>> print('Resampled dataset shape %s' % Counter(y_res))\n",
    "    Resampled dataset shape Counter({{0: 900, 1: 881}})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5da30be9",
   "metadata": {},
   "source": [
    "    Examples\n",
    "    --------\n",
    "    >>> from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "    >>> from sklearn.datasets import make_classification\n",
    "    >>>\n",
    "    >>> X, y = make_classification(n_samples=1000, n_classes=3,\n",
    "    ...                            n_informative=4, weights=[0.2, 0.3, 0.5],\n",
    "    ...                            random_state=0)\n",
    "    >>> clf = BalancedRandomForestClassifier(max_depth=2, random_state=0)\n",
    "    >>> clf.fit(X, y)  # doctest: +ELLIPSIS\n",
    "    BalancedRandomForestClassifier(...)\n",
    "    >>> print(clf.feature_importances_)  # doctest: +ELLIPSIS\n",
    "    [...]\n",
    "    >>> print(clf.predict([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "    ...                     0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n",
    "    [1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
